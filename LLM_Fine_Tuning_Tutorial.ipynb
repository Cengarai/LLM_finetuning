{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Your First LLM: A Hands-On Tutorial with LoRA\n",
    "\n",
    "**Welcome to the complete hands-on tutorial for fine-tuning language models!**\n",
    "\n",
    "In this notebook, we'll walk through the entire process of fine-tuning a small open-source LLM using LoRA (Low-Rank Adaptation). This is perfect for learning, teaching, and practicing LLM customization.\n",
    "\n",
    "## What We'll Cover:\n",
    "1. Environment setup\n",
    "2. Model and dataset selection\n",
    "3. Data preprocessing\n",
    "4. LoRA configuration\n",
    "5. Training\n",
    "6. Testing and inference\n",
    "7. Saving models\n",
    "\n",
    "Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Environment Setup\n",
    "\n",
    "First, let's install all the required libraries. Run this cell if you're in Google Colab or if you need to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -q \"transformers>=4.44.0\" \"datasets>=2.19.0\" peft accelerate bitsandbytes sentencepiece\n",
    "\n",
    "# Import all necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Choose Base Model and Configure Settings\n",
    "\n",
    "We'll start with a small model for faster training and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_MODEL = \"distilgpt2\"  # Small & fast for demos\n",
    "EOS_TOKEN = \"</s>\"         # End-of-sequence token\n",
    "MAX_LEN = 512              # Maximum sequence length\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   Base Model: {BASE_MODEL}\")\n",
    "print(f\"   EOS Token: {EOS_TOKEN}\")\n",
    "print(f\"   Max Length: {MAX_LEN}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "We'll use the Alpaca dataset, which contains instruction-response pairs perfect for teaching models to follow directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Alpaca dataset\n",
    "print(\"ðŸ“š Loading Alpaca dataset...\")\n",
    "ds = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "\n",
    "print(f\"Dataset structure: {ds}\")\n",
    "print(f\"Number of training examples: {len(ds['train'])}\")\n",
    "\n",
    "# Let's look at a few examples\n",
    "print(\"\\nðŸ” Sample examples:\")\n",
    "for i in range(3):\n",
    "    example = ds['train'][i]\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Instruction: {example['instruction'][:100]}...\")\n",
    "    print(f\"Input: {example['input'][:50]}...\")\n",
    "    print(f\"Output: {example['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set Up Tokenizer and Data Preprocessing\n",
    "\n",
    "We'll create a consistent prompt template and tokenize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(\"ðŸ”¤ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "\n",
    "# Handle models without explicit pad tokens\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"   Set pad_token = eos_token\")\n",
    "\n",
    "print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"   EOS token ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"   PAD token ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Convert each example into a training-ready format with proper prompt template\"\"\"\n",
    "    inst = example[\"instruction\"].strip()\n",
    "    inp = example.get(\"input\", \"\").strip()\n",
    "    out = example[\"output\"].strip()\n",
    "    \n",
    "    # Create prompt template\n",
    "    if inp:\n",
    "        prompt = f\"### Instruction:\\n{inst}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{inst}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # Complete training text = prompt + response + EOS\n",
    "    text = prompt + out + EOS_TOKEN\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to the dataset\n",
    "print(\"ðŸ”„ Formatting examples...\")\n",
    "formatted = ds[\"train\"].map(format_example, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "# Show a formatted example\n",
    "print(\"\\nðŸ“ Example of formatted text:\")\n",
    "print(\"=\" * 50)\n",
    "print(formatted[0][\"text\"][:500] + \"...\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split\n",
    "print(\"ðŸ“Š Creating train/validation split...\")\n",
    "split = formatted.train_test_split(test_size=0.01, seed=42)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"   Training examples: {len(train_ds)}\")\n",
    "print(f\"   Validation examples: {len(val_ds)}\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"Convert text to tokens with proper padding/truncation\"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "# Tokenize both datasets\n",
    "print(\"ðŸ”¢ Tokenizing datasets...\")\n",
    "tokenized_train = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val = val_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(\"   âœ… Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(batch):\n",
    "    \"\"\"For causal LM, labels are input_ids with padding tokens masked out\"\"\"\n",
    "    labels = np.array(batch[\"input_ids\"])\n",
    "    # Mask padding tokens in labels (set to -100 so they're ignored in loss calculation)\n",
    "    labels[np.array(batch[\"attention_mask\"]) == 0] = -100\n",
    "    batch[\"labels\"] = labels.tolist()\n",
    "    return batch\n",
    "\n",
    "# Add labels to both datasets\n",
    "print(\"ðŸ·ï¸ Adding labels...\")\n",
    "tokenized_train = tokenized_train.map(add_labels, batched=True)\n",
    "tokenized_val = tokenized_val.map(add_labels, batched=True)\n",
    "\n",
    "# Verify the data structure\n",
    "print(f\"   Dataset features: {tokenized_train.features}\")\n",
    "print(f\"   First example shape - input_ids: {len(tokenized_train[0]['input_ids'])}, labels: {len(tokenized_train[0]['labels'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Base Model and Configure LoRA\n",
    "\n",
    "Now we'll load our base model and set up LoRA for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up quantization for memory efficiency (if available)\n",
    "bnb_kwargs = {}\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        bnb_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        print(\"âœ… 8-bit quantization enabled\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Quantization not available: {e}\")\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "# Load the base model\n",
    "print(f\"ðŸ¤– Loading base model: {BASE_MODEL}...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=False,\n",
    "    device_map=device_map,\n",
    "    **bnb_kwargs\n",
    ")\n",
    "\n",
    "# Prepare for low-bit fine-tuning if quantized\n",
    "if bnb_kwargs:\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    print(\"   âœ… Model prepared for k-bit training\")\n",
    "\n",
    "print(f\"   Model loaded on: {base_model.device}\")\n",
    "print(f\"   Model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "print(\"âš™ï¸ Configuring LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank of adaptation matrices (lower = fewer params)\n",
    "    lora_alpha=32,           # Scaling factor (usually 2x the rank)\n",
    "    target_modules=[\"c_attn\"],  # Which layers to adapt (GPT-2 specific)\n",
    "    lora_dropout=0.05,       # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap base model with LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nðŸ“Š LoRA Configuration:\")\n",
    "print(f\"   Rank (r): {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Target modules: {lora_config.target_modules}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Training Configuration\n",
    "\n",
    "Let's configure our training parameters for optimal learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilgpt2-alpaca-lora\",\n",
    "    num_train_epochs=1,                  # Start with 1 epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,       # Effective batch size = 16\n",
    "    warmup_ratio=0.03,\n",
    "    learning_rate=2e-4,                  # LoRA can handle higher learning rates\n",
    "    weight_decay=0.0,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,  # Ampere+ GPUs\n",
    "    fp16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] < 8,   # Older GPUs\n",
    "    report_to=\"none\",                    # Disable wandb/tensorboard\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ Training Configuration:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size per device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Mixed precision: BF16={training_args.bf16}, FP16={training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer initialized successfully!\")\n",
    "print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "print(f\"   Validation samples: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model!\n",
    "\n",
    "Now for the exciting part - let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "print(\"This may take a while depending on your hardware.\")\n",
    "print(\"Watch the loss values - they should decrease over time.\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Your Fine-Tuned Model\n",
    "\n",
    "Let's see how well your model follows instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text generation pipeline\n",
    "print(\"ðŸ”® Setting up inference pipeline...\")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def build_prompt(instruction, inp=\"\"):\n",
    "    \"\"\"Create properly formatted prompts for inference\"\"\"\n",
    "    if inp:\n",
    "        return f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "print(\"âœ… Pipeline ready for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample instructions\n",
    "test_instructions = [\n",
    "    \"Explain the Moon's phases in one friendly paragraph for a 10-year-old.\",\n",
    "    \"Write a short poem about programming.\",\n",
    "    \"List three benefits of exercise.\",\n",
    "    \"Explain what machine learning is in simple terms.\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing the fine-tuned model:\\n\")\n",
    "\n",
    "for i, instruction in enumerate(test_instructions, 1):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test {i}: {instruction}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    prompt = build_prompt(instruction)\n",
    "    \n",
    "    outputs = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Extract just the response (everything after the prompt)\n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Interactive Testing\n",
    "\n",
    "Try your own instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_interactive(instruction, input_text=\"\", max_tokens=128, temperature=0.7):\n",
    "    \"\"\"Interactive function to test the model with custom instructions\"\"\"\n",
    "    prompt = build_prompt(instruction, input_text)\n",
    "    \n",
    "    outputs = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    generated_text = outputs[0][\"generated_text\"]\n",
    "    response = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"ðŸ¤– Model Response:\")\n",
    "    print(f\"{response}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage - modify these as you like!\n",
    "print(\"ðŸŽ® Interactive Testing - Try your own instructions!\\n\")\n",
    "\n",
    "# Uncomment and modify these lines to test with your own instructions:\n",
    "# test_model_interactive(\"Write a haiku about artificial intelligence\")\n",
    "# test_model_interactive(\"Explain quantum computing to a child\")\n",
    "# test_model_interactive(\"Give me 3 cooking tips for beginners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save Your Fine-Tuned Model\n",
    "\n",
    "Let's save our work so we can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter (lightweight option)\n",
    "print(\"ðŸ’¾ Saving LoRA adapter...\")\n",
    "adapter_path = \"distilgpt2-alpaca-lora/adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "print(f\"   âœ… Adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"   âœ… Tokenizer saved to: {adapter_path}\")\n",
    "\n",
    "# Check saved files\n",
    "import os\n",
    "saved_files = os.listdir(adapter_path)\n",
    "print(f\"\\nðŸ“ Saved files: {saved_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Merge and save full model (larger but standalone)\n",
    "print(\"ðŸ”„ Creating merged model (optional)...\")\n",
    "\n",
    "try:\n",
    "    # Merge LoRA weights with base model\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_path = \"distilgpt2-alpaca-merged\"\n",
    "    merged_model.save_pretrained(merged_path)\n",
    "    tokenizer.save_pretrained(merged_path)\n",
    "    \n",
    "    print(f\"   âœ… Merged model saved to: {merged_path}\")\n",
    "    \n",
    "    # Check file sizes\n",
    "    def get_folder_size(path):\n",
    "        total = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                total += os.path.getsize(os.path.join(dirpath, filename))\n",
    "        return total / (1024 * 1024)  # MB\n",
    "    \n",
    "    adapter_size = get_folder_size(adapter_path)\n",
    "    merged_size = get_folder_size(merged_path)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š File sizes:\")\n",
    "    print(f\"   Adapter only: {adapter_size:.1f} MB\")\n",
    "    print(f\"   Merged model: {merged_size:.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸ Could not create merged model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Loading Your Saved Model\n",
    "\n",
    "Here's how to load your fine-tuned model later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load your saved model later\n",
    "print(\"ðŸ“– How to load your saved model:\")\n",
    "\n",
    "loading_code = '''\n",
    "# To load the LoRA adapter:\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"distilgpt2-alpaca-lora/adapter\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2-alpaca-lora/adapter\")\n",
    "\n",
    "# Or, if you saved the merged model:\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2-alpaca-merged\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2-alpaca-merged\")\n",
    "'''\n",
    "\n",
    "print(loading_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned your first language model! Here's what you accomplished:\n",
    "\n",
    "âœ… **Loaded and preprocessed** a real instruction dataset  \n",
    "âœ… **Configured LoRA** for efficient fine-tuning  \n",
    "âœ… **Trained a language model** to follow instructions  \n",
    "âœ… **Generated text** with your custom model  \n",
    "âœ… **Saved your work** for future use  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics, here are some ideas to explore:\n",
    "\n",
    "### ðŸš€ Scale Up\n",
    "- Try larger models like `TinyLlama/TinyLlama-1.1B-Chat-v1.0`\n",
    "- Increase sequence length to 1024 or 2048 tokens\n",
    "- Train for more epochs (2-3)\n",
    "\n",
    "### ðŸŽ›ï¸ Experiment with LoRA Settings\n",
    "- Higher rank for complex tasks: `r = 32` or `r = 64`\n",
    "- Different target modules for other architectures\n",
    "- Adjust learning rates between `1e-4` and `5e-4`\n",
    "\n",
    "### ðŸ“Š Custom Datasets\n",
    "- Fine-tune on your own instruction data\n",
    "- Try domain-specific datasets (code, medical, legal)\n",
    "- Create conversational datasets for chatbots\n",
    "\n",
    "### ðŸ› ï¸ Advanced Features\n",
    "- Multi-GPU training with `DataParallel`\n",
    "- Evaluation metrics like BLEU or ROUGE\n",
    "- Model deployment with FastAPI or Gradio\n",
    "- Push models to Hugging Face Hub\n",
    "\n",
    "## Resources for Continued Learning\n",
    "\n",
    "- [Hugging Face Course](https://huggingface.co/course/)\n",
    "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft/)\n",
    "- [Community Discussions](https://discuss.huggingface.co/)\n",
    "\n",
    "**Happy fine-tuning! ðŸ¤–âœ¨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting Section\n",
    "\n",
    "If you encounter issues, here are common solutions:\n",
    "\n",
    "### CUDA Out of Memory\n",
    "```python\n",
    "# Reduce batch size\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Or reduce sequence length\n",
    "MAX_LEN = 256\n",
    "\n",
    "# Or use gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "```\n",
    "\n",
    "### Poor Generation Quality\n",
    "```python\n",
    "# Adjust generation parameters\n",
    "temperature = 0.8  # Higher for more creativity\n",
    "top_p = 0.95      # Higher for more diversity\n",
    "max_new_tokens = 256  # More tokens for complete responses\n",
    "```\n",
    "\n",
    "### Model Not Following Instructions\n",
    "- Check that your prompt format matches training exactly\n",
    "- Train for more epochs\n",
    "- Verify your dataset quality\n",
    "- Try a larger LoRA rank (`r = 32`)\n",
    "\n",
    "### Slow Training\n",
    "- Enable mixed precision (`bf16=True` or `fp16=True`)\n",
    "- Use gradient accumulation instead of larger batch sizes\n",
    "- Consider using a smaller model for experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}